{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f7ef388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"RPY2_CFFI_MODE\"] = \"ABI\"\n",
    "import rpy2.robjects as ro\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "from rpy2.rinterface_lib.callbacks import logger as rpy2_logger\n",
    "rpy2_logger.setLevel(\"ERROR\")   # 只显示错误，屏蔽 message 和 warning\n",
    "rpy2_logger.propagate = False   # 阻止继续传给 root logger\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "# from scSurvival.scsurvival import scSurvival, scSurvivalRun, PredictIndSample\n",
    "from scSurvival_beta import scSurvivalRun, PredictIndSample\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import scanpy as sc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "import io\n",
    "import contextlib\n",
    "f = io.StringIO()\n",
    "from lifelines.utils import concordance_index\n",
    "from scipy.stats import percentileofscore\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9ae14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_r_packages():\n",
    "    ro.r('''\n",
    "    rm(list=ls())\n",
    "    # library(\"scater\")\n",
    "    library(\"splatter\")\n",
    "    library(scran)\n",
    "    library(Seurat)\n",
    "    # library(preprocessCore)\n",
    "    library(pROC)\n",
    "    # library(APML2)\n",
    "    # library(APML1)\n",
    "    # library(APML0)\n",
    "\n",
    "    library(ggplot2)\n",
    "    library(dplyr)\n",
    "    library(caret)\n",
    "    set.seed(1)\n",
    "    ''')\n",
    "\n",
    "\n",
    "def simulated_base_sc_dataset(seed=42, plot=False, cell_surv_ratio=0.15):\n",
    "    ro.r(f'''\n",
    "    seed <- {seed}\n",
    "    alpha = {cell_surv_ratio}\n",
    "    sim.groups <- splatSimulateGroups(\n",
    "    batchCells = 10000, nGenes=5000,\n",
    "    #group.prob = c(0.9, 0.05, 0.05),\n",
    "    group.prob = c(1 - 2*alpha, alpha, alpha),\n",
    "    de.prob = c(0.2, 0.06, 0.06), \n",
    "    de.facLoc = c(0.1, 0.1, 0.1),\n",
    "    de.facScale = 0.4,\n",
    "    seed = seed)\n",
    "\n",
    "    data <- CreateSeuratObject(counts = counts(sim.groups), project = 'Scissor_Single_Cell')\n",
    "    data <- AddMetaData(object = data, metadata = sim.groups$Group, col.name = \"sim.group\")\n",
    "    data$Actual.cond <- recode(data$sim.group,'Group1'='other', 'Group2'='good.survival', 'Group3'='bad.survival')\n",
    "\n",
    "    select_gene_ids <- 1:2000\n",
    "    data <- NormalizeData(object = data, normalization.method = \"LogNormalize\", \n",
    "                          scale.factor = 10000)\n",
    "    data <- FindVariableFeatures(object = data, selection.method = 'vst', nfeatures=2000)\n",
    "    var_features_genes = VariableFeatures(data)\n",
    "    ''')\n",
    "\n",
    "    if plot:\n",
    "        ro.r('''\n",
    "        data <- ScaleData(object = data)\n",
    "        data <- RunPCA(object = data, features = VariableFeatures(data)[select_gene_ids])\n",
    "\n",
    "        data <- RunUMAP(object = data, dims = 1:10, n.neighbors = 5, min.dist=0.5, spread=1.5)\n",
    "        # data <- RunUMAP(object = data, dims = 1:10)\n",
    "        data <- FindNeighbors(object = data, dims = 1:10, k.param=20)\n",
    "        # data <- FindNeighbors(object = data, dims = 1:10, k.param=20,prune.SNN = 0.2)\n",
    "        data <- FindClusters(object = data, resolution = 0.5)\n",
    "\n",
    "        DimPlot(object = data, reduction = 'umap', group.by = 'seurat_clusters', label = F, label.size = 10,pt.size=0.5)\n",
    "        ggsave(paste0(save_path, 'simu_seurat_cluster_umap.pdf'), height = 5, width = 7)\n",
    "\n",
    "        DimPlot(object = data, reduction = 'umap', group.by = 'sim.group', pt.size = 0.5, label = T)\n",
    "        ggsave(paste0(save_path, 'simu_group_umap.pdf'), height = 5, width = 7)\n",
    "\n",
    "        # DimPlot(object = data, reduction = 'umap',  cols = c('grey','blue', 'red'), group.by = 'sim.group', pt.size = 0.5, label = T)\n",
    "        # \n",
    "        DimPlot(object = data, reduction = 'umap',  cols = c('grey','blue', 'red'), group.by = 'Actual.cond', pt.size = 0.5, label = T)\n",
    "        ggsave(paste0(save_path, 'simu_surv_group_umap.pdf'), height = 5, width = 7)\n",
    "        ''')\n",
    "\n",
    "def simulated_sc_datasets(plot=False, plot_surv=False, censor_prob=0.1, tr='inf', group_effect=3, high_group_prob=0.5):\n",
    "    ro.globalenv['censor_prob'] = censor_prob\n",
    "    ro.globalenv['tr'] = tr\n",
    "    ro.globalenv['group_effect'] = group_effect\n",
    "    ro.globalenv['high_group_prob'] = high_group_prob\n",
    "    ro.r('''\n",
    "    sim_event_time <- function(s, tr, g=0) {\n",
    "      alpha = 10  # 尺度参数\n",
    "      gamma_shape = 1.5  # 形状参数\n",
    "\n",
    "      if (tr == 'inf'){\n",
    "        beta_aft = log(20)\n",
    "        U = 0.5\n",
    "      } else{\n",
    "        beta_aft = log(tr)\n",
    "        U = runif(1, 0.2, 0.8) # 避免极端值\n",
    "      }\n",
    "      eta <- (beta_aft + g) * s * 2\n",
    "      T = exp(-eta) * (alpha * (-log(U))**(1/gamma_shape))\n",
    "      return(T)\n",
    "    }\n",
    "         \n",
    "    Expression_pbmc <- as.matrix(data@assays[[\"RNA\"]]@layers[[\"data\"]])\n",
    "    rownames(Expression_pbmc) <- rownames(data)\n",
    "    colnames(Expression_pbmc) <- colnames(data)\n",
    "    Expression_pbmc <- as.data.frame(Expression_pbmc)\n",
    "    all_genes <- rownames(Expression_pbmc)\n",
    "         \n",
    "    set.seed(seed)\n",
    "    sampled_cells = 1000\n",
    "    bulk_num=100\n",
    "\n",
    "    other_cells <- colnames(Expression_pbmc)[data$Actual.cond=='other']\n",
    "    good_cells <- colnames(Expression_pbmc)[data$Actual.cond=='good.survival']\n",
    "    bad_cells <- colnames(Expression_pbmc)[data$Actual.cond=='bad.survival']\n",
    "    num_good <- length(good_cells)\n",
    "    num_bad <- length(bad_cells)\n",
    "    bulk_condition = NULL\n",
    "    \n",
    "    # censor_prob\n",
    "    # gamma = log(tr)  # 3\n",
    "    # group_effect = 3 # 0, 1, 3\n",
    "    # high_group_prob = 0.5 # 0.3, 0.5, 0.7\n",
    "\n",
    "    status = NULL\n",
    "    surv_time = NULL\n",
    "\n",
    "    num_good_cond_cells = NULL\n",
    "    num_bad_cond_cells = NULL\n",
    "    group_labels = NULL\n",
    "\n",
    "    sc_data_list = list()\n",
    "    pb <- txtProgressBar(min = 1, max = bulk_num, style = 3)\n",
    "    for (i in 1:bulk_num){\n",
    "      setTxtProgressBar(pb, i)\n",
    "      ratio <- (i-1) / (bulk_num-1)\n",
    "      # ratio <- plogis((ratio - 0.5) * 2 * 6)\n",
    "      num_good_cond_cells_i = round(num_good * ratio)\n",
    "      num_bad_cond_cells_i = round(num_bad * (1-ratio))\n",
    "      condition_good_cells <- good_cells[sample(num_good, num_good_cond_cells_i , replace=TRUE)]\n",
    "      condition_bad_cells <- bad_cells[sample(num_bad, num_bad_cond_cells_i, replace=TRUE)]\n",
    "      condition_cells <- c(condition_good_cells, condition_bad_cells, other_cells)\n",
    "      # condition_cells <- c(condition_bad_cells, other_cells)\n",
    "  \n",
    "      num_good_cond_cells = c(num_good_cond_cells, num_good_cond_cells_i)\n",
    "      num_bad_cond_cells = c(num_bad_cond_cells, num_bad_cond_cells_i)\n",
    "  \n",
    "      Expression_condition = Expression_pbmc[, condition_cells]\n",
    "      Expression_selected <- Expression_condition[, sample(ncol(Expression_condition),size=sampled_cells,replace=TRUE)]\n",
    "  \n",
    "      # filter_cells = intersect(c(condition_bad_cells, other_cells), colnames(Expression_selected))\n",
    "      # Expression_selected <- Expression_selected[, filter_cells]\n",
    "  \n",
    "      # write.csv(Expression_selected, file = sprintf('./source_data/single_cell_revision/%d.csv', i))\n",
    "      sc_data_list[[sprintf('bulk%d', i)]] <- Expression_selected\n",
    "\n",
    "      group_id <- rbinom(1, 1, high_group_prob)\n",
    "      if (group_id == 1){\n",
    "        surv_time_i = sim_event_time((1-ratio), tr, g=group_effect)\n",
    "      }else{\n",
    "        surv_time_i = sim_event_time((1-ratio), tr, g=0)\n",
    "      }\n",
    "      \n",
    "      group_labels <- c(group_labels, group_id)\n",
    "         \n",
    "      if (runif(1, min = 0, max = 1) < censor_prob){\n",
    "        status = c(status, 0)\n",
    "        C = runif(1, min = 0, max = surv_time_i)\n",
    "        surv_time = c(surv_time, C)\n",
    "      }\n",
    "      else{\n",
    "        surv_time = c(surv_time, surv_time_i)\n",
    "        status = c(status, 1)\n",
    "      }\n",
    "    }\n",
    "\n",
    "    bulk_names <- paste0('bulk', 1:bulk_num)\n",
    "    surv_info <- data.frame(\n",
    "      time=surv_time,\n",
    "      status=status,\n",
    "      group=factor(group_labels, levels = c(0, 1), labels = c(\"Young\", \"Old\")),\n",
    "      num.good.cells = num_good_cond_cells,\n",
    "      num.bad.cells = num_bad_cond_cells,\n",
    "      row.names = bulk_names\n",
    "    )\n",
    "\n",
    "    dim(surv_info)\n",
    "    dim(Expression_pbmc)\n",
    "         \n",
    "    labels <- data$Actual.cond\n",
    "    labels <- as.data.frame(labels)\n",
    "    row.names(labels) <- colnames(data)\n",
    "    \n",
    "    ''')\n",
    "\n",
    "    if plot:\n",
    "        ro.r('''\n",
    "        library(gridExtra)\n",
    "        library(ggpubr)\n",
    "\n",
    "        plot_list <- list()\n",
    "\n",
    "        for (i in c(2, 10, 40, 60, 90, 99)){\n",
    "          ratio <- (i-1) / (bulk_num-1)\n",
    "          # ratio <- plogis((ratio - 0.5) * 2 * 6)\n",
    "          num_good_cond_cells_i = round(num_good * ratio)\n",
    "          num_bad_cond_cells_i = round(num_bad * (1-ratio))\n",
    "          condition_good_cells <- good_cells[sample(num_good, num_good_cond_cells_i , replace=TRUE)]\n",
    "          condition_bad_cells <- bad_cells[sample(num_bad, num_bad_cond_cells_i, replace=TRUE)]\n",
    "          condition_cells <- c(condition_good_cells, condition_bad_cells, other_cells)\n",
    "          # condition_cells <- c(condition_bad_cells, other_cells)\n",
    "  \n",
    "  \n",
    "          p <- DimPlot(data[, condition_cells], group.by = 'Actual.cond', cols = c('grey','blue', 'red'), pt.size = 0.5) +\n",
    "          ggtitle(sprintf(\"survival.time: %d months\", i))\n",
    "          plot_list[[length(plot_list) + 1]] <- p\n",
    "        }\n",
    "\n",
    "        # combined_plot <- do.call(grid.arrange, c(plot_list, ncol = 3))\n",
    "        # combined_plot\n",
    "        ggarrange(plotlist = plot_list, ncol = 3, nrow=2, common.legend = TRUE, legend = \"bottom\")\n",
    "        ggsave(paste0(save_path, 'survival.time.simulated.pdf'), height = 7, width = 10.5)\n",
    "        ''')\n",
    "      \n",
    "    if plot_surv:\n",
    "        ro.r('''\n",
    "        library(ggplot2)\n",
    "        library(ggnewscale)\n",
    "        # 示例数据：\n",
    "\n",
    "        df <- data.frame(\n",
    "          id = factor(rownames(surv_info), levels = rownames(surv_info)),\n",
    "          start_time = rep(0, dim(surv_info)[1]),\n",
    "          end_time = surv_info$time,          # 事件或删失时间\n",
    "          event = surv_info$status,             # 1=事件发生, 0=删失\n",
    "          group = surv_info$group\n",
    "        )\n",
    "\n",
    "\n",
    "        # 把event列转为因子，更清晰地显示legend\n",
    "        df$event <- factor(df$event, levels = c(1, 0), labels = c(\"Event\", \"Censored\"))\n",
    "        # df$group <- factor(df$group, levels = c(0, 1), labels = c(\"Young\", \"Old\"))\n",
    "\n",
    "        ggplot(df, aes(x = start_time, xend = end_time, y = id, yend = id)) +\n",
    "          # 线段：按 group 上色\n",
    "          geom_segment(aes(color = group), size = 1) +\n",
    "  \n",
    "          # 点：同时用 color 和 shape 表示 event\n",
    "          geom_point(aes(x = end_time, y = id, color = event, shape = event),\n",
    "                     size = 2, stroke = 1.5) +\n",
    "  \n",
    "          # Event/Censored 同时定义颜色和形状\n",
    "          scale_color_manual(values = c(\"Event\" = \"blue\", \"Censored\" = \"red\"),\n",
    "                             breaks = c(\"Event\", \"Censored\"),\n",
    "                             name = \"Status\") +\n",
    "          scale_shape_manual(values = c(\"Event\" = 16, \"Censored\" = 4),\n",
    "                             breaks = c(\"Event\", \"Censored\"),\n",
    "                             name = \"Status\") +\n",
    "  \n",
    "          # Group 单独一套图例\n",
    "          ggnewscale::new_scale_color() +\n",
    "          geom_segment(aes(color = group), size = 1) +\n",
    "          scale_color_manual(values = c(\"Young\" = \"darkgreen\", \"Old\" = \"orange\"),\n",
    "                             name = \"Group\") +\n",
    "          scale_y_discrete(breaks = function(x) x[seq(1, length(x), by = 5)]) +\n",
    "          labs(x = \"Time\", y = \"Patient ID\") +\n",
    "          theme_minimal(base_size = 14) +\n",
    "          theme(\n",
    "            panel.grid = element_blank(),\n",
    "            axis.text.x = element_text(angle = 45, hjust = 1),\n",
    "            axis.line.x = element_line(size = 0.8, color = \"black\"),\n",
    "            axis.line.y = element_line(size = 0.8, color = \"black\"),\n",
    "            axis.ticks.x = element_line(size = 0.8, color = \"black\"),\n",
    "            axis.ticks.y = element_line(size = 0.8, color = \"black\"),\n",
    "            legend.position = \"top\"\n",
    "          ) +\n",
    "          coord_flip(clip = \"off\")\n",
    "\n",
    "\n",
    "        ggsave(paste0(save_path, 'Survival_Data_plot.pdf'), width = 6, height = 4)\n",
    "        ''')\n",
    "\n",
    "    # collected sc_data_list, surv_info, Expression_pbmc and transfer to python\n",
    "    surv_info_df     = r_to_pandas(\"surv_info\")\n",
    "    Expression_pbmc_df = r_to_pandas(\"Expression_pbmc\")\n",
    "    sc_data_list     = r_list_to_pydict_df(\"sc_data_list\")  # dict: { 'bulk_1': DataFrame, ... }\n",
    "    labels_df       = r_to_pandas(\"labels\")\n",
    "    features = {\n",
    "        'all_genes': list(ro.r(\"all_genes\")),\n",
    "        'hvg': list(ro.r(\"var_features_genes\"))\n",
    "    }\n",
    "\n",
    "    return_data = {\n",
    "        'sc_data_list': sc_data_list,\n",
    "        'surv_info_df': surv_info_df,\n",
    "        'Expression_pbmc_df': Expression_pbmc_df,\n",
    "        'labels_df': labels_df,\n",
    "        'features': features\n",
    "    }\n",
    "\n",
    "    return return_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5473ad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_data_for_model(datasets):\n",
    "    sc_data_list = datasets['sc_data_list']\n",
    "    clinic = datasets['surv_info_df']\n",
    "\n",
    "    xs = []\n",
    "    samples = []\n",
    "    for key, val in tqdm(sc_data_list.items()):\n",
    "        df = val\n",
    "        xs.append(df.values.T)\n",
    "        samples.extend([key] * df.shape[1])\n",
    "\n",
    "    X = np.concatenate(xs, axis=0)\n",
    "    adata = sc.AnnData(X, obs=pd.DataFrame(samples, index=np.arange(X.shape[0]), columns=['sample']),\n",
    "    var=pd.DataFrame(index=datasets['features']['all_genes']))\n",
    "\n",
    "    adata.raw = adata.copy()\n",
    "    adata = adata[:, datasets['features']['hvg']]\n",
    "\n",
    "    surv = clinic[['time', 'status', 'group']].copy()\n",
    "    surv['time'] = surv['time'].astype(float)\n",
    "    surv['status'] = surv['status'].astype(int)\n",
    "\n",
    "    df = datasets['Expression_pbmc_df']\n",
    "    x = df.values.T\n",
    "    sim_group = datasets['labels_df']\n",
    "    sim_group = sim_group['labels'].values\n",
    "\n",
    "    adata_new = sc.AnnData(x, obs=pd.DataFrame(sim_group, index=np.arange(x.shape[0]), columns=['sim_group']), var=pd.DataFrame(index=datasets['features']['all_genes']))\n",
    "\n",
    "    return adata, surv, adata_new\n",
    "\n",
    "def detect_subpopulations(adata, surv, adata_new, entropy_threshold=0.7, save_path=None):\n",
    "    covariates = surv[['group']].copy()\n",
    "    surv = surv[['time', 'status']].copy()\n",
    "    adata, surv, model = scSurvivalRun(adata, \n",
    "        sample_column='sample',\n",
    "        surv=surv,\n",
    "        # batch_key='batch',\n",
    "        covariates=covariates,\n",
    "        feature_flavor='AE',\n",
    "        entropy_threshold=entropy_threshold,\n",
    "        lambdas=(0.01, 1.0),\n",
    "        pretrain_epochs=200,\n",
    "        epochs=500,\n",
    "        weight_decay=0.01,\n",
    "        lr=0.001,\n",
    "        patience=100,\n",
    "        rec_likelihood='ZIG',\n",
    "        do_scale_ae=False,\n",
    "        beta=0.1, tau=0.2, \n",
    "        sample_size_ae=None,\n",
    "        finetue_lr_factor=0.1,\n",
    "        gene_weight_alpha=0.2,\n",
    "        gamma_beta_weight=(0.1, 0.0),\n",
    "        once_load_to_gpu=True,\n",
    "        use_amp=False,\n",
    "        fitnetune_strategy='alternating', # jointly, alternating, alternating_lightly,\n",
    "        )\n",
    "\n",
    "    data = adata.obs['attention'].values.reshape(-1, 1)\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "    kmeans.fit(data)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    atten_thr = cluster_centers.flatten().mean()\n",
    "    \n",
    "    adata_new, _ = PredictIndSample(adata_new, adata, model)\n",
    "\n",
    "    attention = adata_new.obs['attention'].values\n",
    "    hazard_adj = adata_new.obs['hazard_adj'].values\n",
    "    hazard = adata_new.obs['hazard'].values\n",
    "\n",
    "    risk_group = np.array(['inattentive'] * attention.shape[0], dtype=object)\n",
    "    risk_group[np.logical_and(attention >= atten_thr, hazard_adj > 0)] = 'higher'\n",
    "    risk_group[np.logical_and(attention >= atten_thr, hazard_adj <= 0)] = 'lower'\n",
    "\n",
    "    # higher -> bad.survival, lower -> good.survival, inattentive -> other \n",
    "\n",
    "    risk_group_recoded = np.array(['other'] * attention.shape[0], dtype=object)\n",
    "    risk_group_recoded[risk_group == 'higher'] = 'bad.survival'\n",
    "    risk_group_recoded[risk_group == 'lower'] = 'good.survival'\n",
    "\n",
    "    clf_report = classification_report(adata_new.obs['sim_group'].values, risk_group_recoded, output_dict=True, zero_division=0)\n",
    "\n",
    "    clf_report_df = pd.DataFrame(clf_report).T\n",
    "\n",
    "    if save_path is not None:\n",
    "        model.covariate_coef.to_csv(os.path.join(save_path, 'covariate_coef.csv'))\n",
    "\n",
    "    return clf_report_df, adata_new\n",
    "\n",
    "def cross_validation_samples(adata, surv, entropy_threshold=0.7):\n",
    "    # 交叉验证样本\n",
    "    adata = adata.raw.to_adata()\n",
    "    adata.obs['patient_no'] = adata.obs['sample']\n",
    "    patients = adata.obs['patient_no'].unique()\n",
    "\n",
    "    # K fold cross validation\n",
    "    cv_hazards_adj_cells = np.zeros((adata.shape[0], ))\n",
    "    surv['cv_hazards_adj_patient'] = 0.0\n",
    "    surv['cv_hazard_percentile_patient'] = 0.0\n",
    "    cindexs = []\n",
    "    surv_test_all_folds = []\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(patients)):\n",
    "\n",
    "        print(f'fold {i}, train_size: {train_index.shape[0]}, test_size: {test_index.shape[0]}')\n",
    "        train_patients = patients[train_index]\n",
    "        test_patients = patients[test_index]\n",
    "\n",
    "        # train\n",
    "        adata_train = adata[adata.obs['patient_no'].isin(train_patients), :].copy()\n",
    "    \n",
    "        ## select HVGs on training set only\n",
    "        sc.pp.highly_variable_genes(adata_train, n_top_genes=2000, subset=False, flavor='seurat')\n",
    "        hvgs = adata_train.var[adata_train.var['highly_variable']].index.tolist() \n",
    "        adata_train = adata_train[:, hvgs]\n",
    "\n",
    "        surv_train = surv.loc[surv.index.isin(train_patients), :].copy()\n",
    "        covariates_train = surv_train[['group']].copy()\n",
    "\n",
    "        adata_train, surv_train, model = scSurvivalRun(\n",
    "            adata_train,\n",
    "            sample_column='sample',\n",
    "            surv=surv_train,\n",
    "            # batch_key='batch',\n",
    "            covariates=covariates_train,\n",
    "            feature_flavor='AE',\n",
    "            entropy_threshold=entropy_threshold,\n",
    "            validate=True,\n",
    "            validate_ratio=0.2,\n",
    "            validate_metric='ccindex',\n",
    "            lambdas=(0.01, 1.0),\n",
    "            pretrain_epochs=200,\n",
    "            epochs=500,\n",
    "            weight_decay=0.01,\n",
    "            lr=0.001,\n",
    "            patience=100,\n",
    "            rec_likelihood='ZIG',\n",
    "            do_scale_ae=False,\n",
    "            beta=0.1, tau=0.2, \n",
    "            sample_size_ae=None,\n",
    "            finetue_lr_factor=0.1,\n",
    "            gene_weight_alpha=0.2,\n",
    "            gamma_beta_weight=(0.1, 0.0),\n",
    "            once_load_to_gpu=True,\n",
    "            use_amp=False,\n",
    "            fitnetune_strategy='alternating', # jointly, alternating, alternating_lightly,\n",
    "            )  \n",
    "        \n",
    "        \n",
    "        train_cindex = concordance_index(surv_train['time'], -surv_train['patient_hazards'], surv_train['status'])\n",
    "        print(f'train c-index: {train_cindex:.4f}')\n",
    "\n",
    "        # test\n",
    "        print('testing...')\n",
    "        adata_test = adata[adata.obs['patient_no'].isin(test_patients), :].copy()\n",
    "        adata_test = adata_test[:, hvgs]\n",
    "\n",
    "        with contextlib.redirect_stdout(f):\n",
    "            for test_patient in test_patients:\n",
    "                adata_test_patient = adata_test[adata_test.obs['patient_no'] == test_patient, :].copy()\n",
    "                covariates_test_patient = surv.loc[[test_patient], ['group']].copy()\n",
    "                adata_test_patient, patient_hazard = PredictIndSample(adata_test_patient, adata_train, model, covariates_new=covariates_test_patient)\n",
    "                cv_hazards_adj_cells[adata.obs['patient_no'] == test_patient] = adata_test_patient.obs['hazard_adj'].values\n",
    "                surv.loc[surv.index == test_patient, 'cv_hazards_adj_patient'] = patient_hazard\n",
    "                surv.loc[surv.index == test_patient, 'cv_hazard_percentile_patient'] = percentileofscore(surv_train['patient_hazards'], patient_hazard, kind='rank')\n",
    "\n",
    "        surv_test = surv.loc[surv.index.isin(test_patients), :]\n",
    "        c_index = concordance_index(surv_test['time'], -surv_test['cv_hazards_adj_patient'], surv_test['status'])\n",
    "\n",
    "        cindexs.append(c_index)\n",
    "        surv_test_all_folds.append(surv_test)\n",
    "\n",
    "        print(f'c-index: {c_index:.4f}')\n",
    "        print('='*50)\n",
    "\n",
    "        # if i == 0:\n",
    "        #     break\n",
    "\n",
    "    mean_cindex = np.mean(cindexs)\n",
    "    std_cindex = np.std(cindexs)\n",
    "\n",
    "    print(f'mean c-index: {mean_cindex:.4f} ± {std_cindex:.4f}')\n",
    "    cindexs_df = pd.DataFrame(cindexs, columns=['c-index'], index=['fold%d' % i for i in range(5)])\n",
    "\n",
    "    cindex_results = {\n",
    "        'mean_cindex': mean_cindex,\n",
    "        'std_cindex': std_cindex,\n",
    "        'cindexs_df': cindexs_df\n",
    "    }\n",
    "\n",
    "    return cindex_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bff430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1/160: seed=1, group_effect=3, high_group_prob=0.5\n",
      "Modularity Optimizer version 1.3.0 by Ludo Waltman and Nees Jan van Eck\n",
      "\n",
      "Number of nodes: 10000\n",
      "Number of edges: 277207\n",
      "\n",
      "Running Louvain algorithm...\n",
      "Maximum modularity in 10 random starts: 0.7719\n",
      "Number of communities: 3\n",
      "Elapsed time: 1 seconds\n",
      "  |======================================================================| 100%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 6172.63it/s]\n",
      "c:\\Users\\renta\\AppData\\Local\\anaconda3\\Lib\\site-packages\\anndata\\_core\\aligned_df.py:67: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "c:\\Users\\renta\\AppData\\Local\\anaconda3\\Lib\\site-packages\\anndata\\_core\\aligned_df.py:67: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "Pretraining: 100%|██████████| 200/200 [05:57<00:00,  1.79s/it, ae_loss=-201] \n",
      "Finetuning:  89%|████████▊ | 443/500 [20:29<02:38,  2.78s/it, ae_loss=-217, atten_entropy=0.684, cox_loss=1.93, loss=-0.233]  \n",
      "d:\\Projects\\scSurvival\\code\\datasets\\sim_surv\\../..\\scSurvival_beta\\scsurvival.py:231: ImplicitModificationWarning: Setting element `.obsm['X_ae']` of view, initializing view as actual.\n",
      "  adata.obsm['X_ae'] = h.cpu().detach().numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added hazard and attention to adata.obs.\n",
      "Added patient_hazards to surv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\renta\\AppData\\Local\\anaconda3\\Lib\\site-packages\\threadpoolctl.py:1226: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene missing rate: 0.00%\n",
      "Added hazard and attention to adata_new.obs.\n"
     ]
    }
   ],
   "source": [
    "from utils import Logger\n",
    "from itertools import product\n",
    "load_r_packages()\n",
    "param_grid = {\n",
    "    'seed': range(1, 11),\n",
    "    'group_effect': [0, 1, 3],\n",
    "    'high_group_prob': [0.3, 0.5, 0.7],\n",
    "}\n",
    "keys, values = zip(*param_grid.items())\n",
    "combos = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "save_root_path = './results/revision-sim5-python/'\n",
    "logger = Logger(save_path=f'{save_root_path}cell_subpopulation_logs.csv')\n",
    "\n",
    "for i, params in enumerate(combos):\n",
    "    # if i > 0:\n",
    "    #     break\n",
    "\n",
    "    logger.log_dict(params)\n",
    "    seed = params['seed']\n",
    "    group_effect = params['group_effect']\n",
    "    high_group_prob = params['high_group_prob']\n",
    "\n",
    "    print(f'Running {i+1}/{len(combos)}: seed={seed}, group_effect={group_effect}, high_group_prob={high_group_prob}')\n",
    "\n",
    "    save_path = f'{save_root_path}/group-effect-{group_effect}_high-group-prob-{high_group_prob}/seed-{seed}/'\n",
    "    ro.globalenv['save_path'] = save_path\n",
    "    \n",
    "    if seed == 1:\n",
    "        ro.r('dir.create(save_path, recursive=T)')\n",
    "        if i == 0:\n",
    "            simulated_base_sc_dataset(seed=seed, plot=True)\n",
    "            datasets = simulated_sc_datasets(plot=True, plot_surv=True, censor_prob=0.1, tr='inf', group_effect=group_effect, high_group_prob=high_group_prob)\n",
    "        else:\n",
    "            simulated_base_sc_dataset(seed=seed, plot=False)\n",
    "            datasets = simulated_sc_datasets(plot=False, plot_surv=True, censor_prob=0.1, tr='inf', group_effect=group_effect, high_group_prob=high_group_prob)\n",
    "    else:\n",
    "        simulated_base_sc_dataset(seed=seed, plot=False)\n",
    "        datasets = simulated_sc_datasets(plot=False, plot_surv=False, censor_prob=0.1, tr='inf', group_effect=group_effect, high_group_prob=high_group_prob)\n",
    "\n",
    "    adata, surv, adata_new = organize_data_for_model(datasets)\n",
    "    if seed == 1:\n",
    "        clf_report_df, adata_new = detect_subpopulations(adata, surv, adata_new, entropy_threshold=0.7, save_path=save_path)\n",
    "    else:\n",
    "        clf_report_df, adata_new = detect_subpopulations(adata, surv, adata_new, entropy_threshold=0.7, save_path=None)\n",
    "\n",
    "    clf_rst = {\n",
    "        'precision': clf_report_df.loc['macro avg', 'precision'],\n",
    "        'recall': clf_report_df.loc['macro avg', 'recall'],\n",
    "        'f1-score': clf_report_df.loc['macro avg', 'f1-score'],\n",
    "    }\n",
    "\n",
    "    for cls in ['good.survival', 'bad.survival', 'other']:\n",
    "        for metric in ['precision', 'recall', 'f1-score']:\n",
    "            key = f'{cls}_{metric}'\n",
    "            if cls in clf_report_df.index:\n",
    "                clf_rst[key] = clf_report_df.loc[cls, metric]\n",
    "            else:\n",
    "                clf_rst[key] = 0.0\n",
    "\n",
    "    logger.log_dict(clf_rst)\n",
    "    logger.get_logs_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b4ed98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1/160: seed=1, group_effect=3, high_group_prob=0.5\n",
      "  |======================================================================| 100%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 33333.10it/s]\n",
      "c:\\Users\\renta\\AppData\\Local\\anaconda3\\Lib\\site-packages\\anndata\\_core\\aligned_df.py:67: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "c:\\Users\\renta\\AppData\\Local\\anaconda3\\Lib\\site-packages\\anndata\\_core\\aligned_df.py:67: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0, train_size: 80, test_size: 20\n",
      "Validation mode is enabled, will split 20% of the data for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretraining: 100%|██████████| 200/200 [03:35<00:00,  1.08s/it, ae_loss=-107]  \n",
      "Finetuning:  97%|█████████▋| 485/500 [14:48<00:27,  1.83s/it, ae_loss=-125, atten_entropy=0.684, ccindex_val=0.895, cox_loss=1.59, loss=0.339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping with best validation ccindex: 0.9044.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\scSurvival\\code\\datasets\\sim_surv\\../..\\scSurvival_beta\\scsurvival.py:231: ImplicitModificationWarning: Setting element `.obsm['X_ae']` of view, initializing view as actual.\n",
      "  adata.obsm['X_ae'] = h.cpu().detach().numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added hazard and attention to adata.obs.\n",
      "Added patient_hazards to surv.\n",
      "train c-index: 0.9616\n",
      "testing...\n",
      "c-index: 0.8192\n",
      "==================================================\n",
      "fold 1, train_size: 80, test_size: 20\n",
      "Validation mode is enabled, will split 20% of the data for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretraining: 100%|██████████| 200/200 [03:32<00:00,  1.06s/it, ae_loss=-117]  \n",
      "Finetuning:   3%|▎         | 17/500 [00:33<15:38,  1.94s/it, ae_loss=-126, atten_entropy=0.998, ccindex_val=0.742, cox_loss=3.14, loss=2.18]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m datasets \u001b[38;5;241m=\u001b[39m simulated_sc_datasets(plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, plot_surv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, censor_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, tr\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m, group_effect\u001b[38;5;241m=\u001b[39mgroup_effect, high_group_prob\u001b[38;5;241m=\u001b[39mhigh_group_prob)\n\u001b[0;32m     29\u001b[0m adata, surv, adata_new \u001b[38;5;241m=\u001b[39m organize_data_for_model(datasets)\n\u001b[1;32m---> 30\u001b[0m cindex_results \u001b[38;5;241m=\u001b[39m cross_validation_samples(adata, surv, entropy_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n\u001b[0;32m     32\u001b[0m cindex_results \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_cindex\u001b[39m\u001b[38;5;124m'\u001b[39m: cindex_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_cindex\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd_cindex\u001b[39m\u001b[38;5;124m'\u001b[39m: cindex_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd_cindex\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     35\u001b[0m }\n\u001b[0;32m     37\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog_dict(cindex_results)\n",
      "Cell \u001b[1;32mIn[15], line 122\u001b[0m, in \u001b[0;36mcross_validation_samples\u001b[1;34m(adata, surv, entropy_threshold)\u001b[0m\n\u001b[0;32m    119\u001b[0m surv_train \u001b[38;5;241m=\u001b[39m surv\u001b[38;5;241m.\u001b[39mloc[surv\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39misin(train_patients), :]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    120\u001b[0m covariates_train \u001b[38;5;241m=\u001b[39m surv_train[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m--> 122\u001b[0m adata_train, surv_train, model \u001b[38;5;241m=\u001b[39m scSurvivalRun(\n\u001b[0;32m    123\u001b[0m     adata_train,\n\u001b[0;32m    124\u001b[0m     sample_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    125\u001b[0m     surv\u001b[38;5;241m=\u001b[39msurv_train,\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;66;03m# batch_key='batch',\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     covariates\u001b[38;5;241m=\u001b[39mcovariates_train,\n\u001b[0;32m    128\u001b[0m     feature_flavor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAE\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    129\u001b[0m     entropy_threshold\u001b[38;5;241m=\u001b[39mentropy_threshold,\n\u001b[0;32m    130\u001b[0m     validate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    131\u001b[0m     validate_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m    132\u001b[0m     validate_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mccindex\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    133\u001b[0m     lambdas\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m1.0\u001b[39m),\n\u001b[0;32m    134\u001b[0m     pretrain_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[0;32m    135\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[0;32m    136\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[0;32m    137\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[0;32m    138\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m    139\u001b[0m     rec_likelihood\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZIG\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    140\u001b[0m     do_scale_ae\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    141\u001b[0m     beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, tau\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, \n\u001b[0;32m    142\u001b[0m     sample_size_ae\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    143\u001b[0m     finetue_lr_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m    144\u001b[0m     gene_weight_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m    145\u001b[0m     gamma_beta_weight\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.0\u001b[39m),\n\u001b[0;32m    146\u001b[0m     once_load_to_gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m     use_amp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    148\u001b[0m     fitnetune_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malternating\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# jointly, alternating, alternating_lightly,\u001b[39;00m\n\u001b[0;32m    149\u001b[0m     )  \n\u001b[0;32m    152\u001b[0m train_cindex \u001b[38;5;241m=\u001b[39m concordance_index(surv_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m-\u001b[39msurv_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatient_hazards\u001b[39m\u001b[38;5;124m'\u001b[39m], surv_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain c-index: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_cindex\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Projects\\scSurvival\\code\\datasets\\sim_surv\\../..\\scSurvival_beta\\scsurvival.py:218\u001b[0m, in \u001b[0;36mscSurvivalRun\u001b[1;34m(adata, sample_column, surv, covariates, batch_key, feature_flavor, beta, tau, hidden_size, num_heads, rec_likelihood, do_scale_ae, gene_weight_alpha, model_save_dir, model_load_dir, dropout, predict_nMC, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m     model\u001b[38;5;241m.\u001b[39mcell_model\u001b[38;5;241m.\u001b[39mfeature_extractor\u001b[38;5;241m.\u001b[39mrecon_logvar\u001b[38;5;241m.\u001b[39mcopy_(torch\u001b[38;5;241m.\u001b[39mtensor(non_zero_vars)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)) \n\u001b[0;32m    216\u001b[0m     model\u001b[38;5;241m.\u001b[39mcell_model\u001b[38;5;241m.\u001b[39mfeature_extractor\u001b[38;5;241m.\u001b[39mrecon_logvar\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(xs, \n\u001b[0;32m    219\u001b[0m           y_time, \n\u001b[0;32m    220\u001b[0m           y_event, \n\u001b[0;32m    221\u001b[0m           covariates_encoded\u001b[38;5;241m=\u001b[39mcovariates_encoded,\n\u001b[0;32m    222\u001b[0m           batch_lists\u001b[38;5;241m=\u001b[39mbatch_lists,\n\u001b[0;32m    223\u001b[0m           feature_weights\u001b[38;5;241m=\u001b[39mgene_weights,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;66;03m#   epochs=500, \u001b[39;00m\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;66;03m#   instance_batch_size=3000, \u001b[39;00m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;66;03m#   lambdas=(1.0, 1.0),\u001b[39;00m\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;66;03m#   entropy_threshold=0.7,\u001b[39;00m\n\u001b[0;32m    228\u001b[0m           \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    230\u001b[0m h, a, cell_hazards, cell_hazards_weighted \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_cells(exp, batch_labels\u001b[38;5;241m=\u001b[39mbatchs)\n\u001b[0;32m    231\u001b[0m adata\u001b[38;5;241m.\u001b[39mobsm[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_ae\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32md:\\Projects\\scSurvival\\code\\datasets\\sim_surv\\../..\\scSurvival_beta\\scsurvival_core.py:330\u001b[0m, in \u001b[0;36mscSurvival.fit\u001b[1;34m(self, xs, y_time, y_event, covariates_encoded, batch_lists, validate, validate_ratio, validate_metric, validate_nMC, feature_weights, epochs, pretrain_epochs, lr, pretrain_batch_size, instance_batch_size, lambdas, entropy_threshold, weight_decay, patience, temperature, finetue_lr_factor, gamma_beta_weight, once_load_to_gpu, use_amp, fitnetune_strategy, sample_balance, **kwargs)\u001b[0m\n\u001b[0;32m    328\u001b[0m     train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    329\u001b[0m     num_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 330\u001b[0m ae_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrain_epoch(all_dataloader, feature_weights, gamma_beta_weight, scaler, amp_context, use_amp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, lamda\u001b[38;5;241m=\u001b[39mlambdas[\u001b[38;5;241m0\u001b[39m], num_iter\u001b[38;5;241m=\u001b[39mnum_iter, train\u001b[38;5;241m=\u001b[39mtrain)  \u001b[38;5;66;03m# jointly train autoencoder\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# phase 2\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32md:\\Projects\\scSurvival\\code\\datasets\\sim_surv\\../..\\scSurvival_beta\\scsurvival_core.py:93\u001b[0m, in \u001b[0;36mscSurvival.pretrain_epoch\u001b[1;34m(self, all_dataloader, feature_weights, gamma_beta_weight, scaler, amp_context, use_amp, optimizer, lamda, num_iter, train)\u001b[0m\n\u001b[0;32m     91\u001b[0m         scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 93\u001b[0m         loss_ae\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     94\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     96\u001b[0m ae_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ae_loss_batch\n",
      "File \u001b[1;32mc:\\Users\\renta\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    523\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\renta\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[0;32m    290\u001b[0m     tensors,\n\u001b[0;32m    291\u001b[0m     grad_tensors_,\n\u001b[0;32m    292\u001b[0m     retain_graph,\n\u001b[0;32m    293\u001b[0m     create_graph,\n\u001b[0;32m    294\u001b[0m     inputs,\n\u001b[0;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    297\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\renta\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    769\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    770\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils import Logger\n",
    "from itertools import product\n",
    "load_r_packages()\n",
    "param_grid = {\n",
    "    'seed': [1],\n",
    "    'group_effect': [0, 1, 3],\n",
    "    'high_group_prob': [0.3, 0.5, 0.7],\n",
    "}\n",
    "keys, values = zip(*param_grid.items())\n",
    "combos = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "save_root_path = './results/revision-sim5-python/'\n",
    "logger = Logger(save_path=f'{save_root_path}cv_logs.csv')\n",
    "\n",
    "for i, params in enumerate(combos):\n",
    "    # if i > 0:\n",
    "    #     break\n",
    "    \n",
    "    logger.log_dict(params)\n",
    "    seed = params['seed']\n",
    "    group_effect = params['group_effect']\n",
    "    high_group_prob = params['high_group_prob']\n",
    "\n",
    "    print(f'Running {i+1}/{len(combos)}: seed={seed}, group_effect={group_effect}, high_group_prob={high_group_prob}')\n",
    "    \n",
    "    simulated_base_sc_dataset(seed=seed, plot=False)\n",
    "    datasets = simulated_sc_datasets(plot=False, plot_surv=False, censor_prob=0.1, tr='inf', group_effect=group_effect, high_group_prob=high_group_prob)\n",
    "\n",
    "    adata, surv, adata_new = organize_data_for_model(datasets)\n",
    "    cindex_results = cross_validation_samples(adata, surv, entropy_threshold=0.7)\n",
    "\n",
    "    cindex_results = {\n",
    "        'mean_cindex': cindex_results['mean_cindex'],\n",
    "        'std_cindex': cindex_results['std_cindex']\n",
    "    }\n",
    "\n",
    "    logger.log_dict(cindex_results)\n",
    "    logger.get_logs_df()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
